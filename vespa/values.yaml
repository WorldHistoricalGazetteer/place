namespace: vespa

replicaCounts:
  # Ensure quorum for ZooKeeper: if you increase this value, you must create additional directories for PVs
  configserver: 3
  admin: 1         # Only one admin node is needed
  content: 1       # TODO: Increase this value for production
  feed: 1          # TODO: Increase this value for production
  query: 1         # TODO: Increase this value for production
  api: 1           # TODO: Increase this value for production

image:
  repository: vespaengine/vespa
  pullPolicy: IfNotPresent
  tag: "8"

imagePullSecrets: [ ]
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: { }
  name: ""

podAnnotations: { }

podSecurityContext:
  fsGroup: 1000

securityContext:
  container:
    runAsUser: 1000
#    fsGroup: 1000

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: contour
  annotations:
    kubernetes.io/ingress.class: contour
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
  hosts:
    - host: index.whgazetteer.org
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls:
    - secretName: vespa-tls

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: { }

tolerations: [ ]

affinity: { }

# MetalLB LoadBalancer configuration (useful if you are exposing services via LoadBalancer)
loadBalancerIP: "auto"  # or specify an IP if you are using MetalLB

flannel:
  enabled: true  # Enable Flannel for networking in your cluster
  cniVersion: "v0.26.1"

certManager:
  enabled: true
  issuerRef:
    name: "letsencrypt-prod"
    kind: ClusterIssuer

resources:
  configserver:
    requests:
      memory: "4G"
    limits:
      memory: "4G"
  admin:
    requests:
      memory: "1G"
    limits:
      memory: "1G"
  content:
    requests:
      memory: "4G"
    limits:
      memory: "4G"
  feed:
    requests:
      memory: "2G"
    limits:
      memory: "8G"
  query:
    requests:
      memory: "2G"
    limits:
      memory: "4G"
  api:
    requests:
      memory: "1G"
    limits:
      memory: "4G"

common:
  initContainer: |
    name: wait-for-config-servers
    image: badouralix/curl-jq:alpine
    imagePullPolicy: IfNotPresent
    command:
      - sh
      - -c
      - |
        echo "Waiting for all config servers to be ready..."
        CONFIG_SERVERS=$(echo "$VESPA_CONFIGSERVERS" | tr ',' '\n')
        for SERVER in $CONFIG_SERVERS; do
          echo "Checking health of $SERVER..."
          RESPONSE=$(curl -sf "http://$SERVER:19071/state/v1/health")
          echo "Response: $RESPONSE"
          until echo "$RESPONSE" | jq -e '.status.code == "up"' > /dev/null; do
            echo "$SERVER not ready. Retrying in 5 seconds..."
            sleep 5
            RESPONSE=$(curl -sf "http://$SERVER:19071/state/v1/health")  # Retry fetching the health response
            echo $RESPONSE
          done
          echo "$SERVER is ready."
        done
        echo "All config servers are ready."
    envFrom:
      - configMapRef:
          name: vespa-config

configserver:
  #  configserverJvmArgs: "-Xms32M -Xmx128M" # TODO: Comment out for production
  #  configproxyJvmArgs: "-Xms32M -Xmx32M" # TODO: Comment out for production
  pvRootPath: "/data/k8s/vespa-config"
  pvs:
    - name: var
      storageSize: 5Gi
      mountPath: /opt/vespa/var
    - name: logs
      storageSize: 5Gi
      mountPath: /opt/vespa/logs
    - name: workspace
      storageSize: 1Gi
      mountPath: /workspace

content:
  pv:
    name: var
    storageSize: 5Gi
    rootPath: "/data/k8s/vespa-content-var"
    mountPath: /opt/vespa/var

nodeAffinities:
  admin:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: vespa-role-admin
              operator: In
              values:
                - "true"
  content:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: vespa-role-content
              operator: In
              values:
                - "true"
ports:
  - containerPort: 8080
  - containerPort: 19090
  - containerPort: 19098
  - containerPort: 19097

ingestion:
  pv:
    name: ingestion
    storageSize: 50Gi
    rootPath: "/data/k8s/vespa-ingestion"
    mountPath: /ingestion

api:
  image:
    repository: worldhistoricalgazetteer/vespa-api
    pullPolicy: IfNotPresent
    tag: "0.0.12"
  securityContext:
    container:
      runAsUser: 0
  git:
    url: "https://github.com/WorldHistoricalGazetteer/place.git"
    sourceFolder: "vespa/repository/"
  containerPort: 8082
  service:
#    type: ClusterIP # Switch to ClusterIP from NodePort for production
    type: NodePort
    port: 8082
    servicePort: 30082
  batchFeedSize: 500
